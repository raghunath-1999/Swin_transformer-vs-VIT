{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9698e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.18\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4787cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\navee\\anaconda3\\envs\\DL_projects\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.12.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0fc6465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db5a15ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296836c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "class_names=['garlic_bread', 'hot_dog', 'ice_cream', 'omelette', 'pizza']\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1f6cbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a class which subclasses nn.Module\n",
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of color channels for the input images. Defaults to 3.\n",
    "        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n",
    "        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n",
    "    \"\"\" \n",
    "    # 2. Initialize the class with appropriate variables\n",
    "    def __init__(self, \n",
    "                 in_channels:int=3,\n",
    "                 patch_size:int=16,\n",
    "                 embedding_dim:int=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3. Create a layer to turn an image into patches\n",
    "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=embedding_dim,\n",
    "                                 kernel_size=patch_size,\n",
    "                                 stride=patch_size,\n",
    "                                 padding=0)\n",
    "\n",
    "        # 4. Create a layer to flatten the patch feature maps into a single dimension\n",
    "        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n",
    "                                  end_dim=3)\n",
    "\n",
    "    # 5. Define the forward method \n",
    "    def forward(self, x):\n",
    "        patch_size=16\n",
    "        # Create assertion to check that inputs are the correct shape\n",
    "        image_resolution = x.shape[-1]\n",
    "        assert image_resolution % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n",
    "        \n",
    "        # Perform the forward pass\n",
    "        x_patched = self.patcher(x)\n",
    "        x_flattened = self.flatten(x_patched) \n",
    "        \n",
    "        # 6. Make sure the output shape has the right order \n",
    "        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8284b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n",
    "    \"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base #12\n",
    "                 attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "        # 4. Create the Multi-Head Attention (MSA) layer\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True) # does our batch dimension come first?\n",
    "        \n",
    "    # 5. Create a forward() method to pass the data throguh the layers\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        attn_output, _ = self.multihead_attn(query=x, # query embeddings \n",
    "                                             key=x, # key embeddings\n",
    "                                             value=x, # value embeddings\n",
    "                                             need_weights=False) # do we need the weights or just the layer outputs?\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4ddf0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# class BN_bnc(nn.BatchNorm1d):\n",
    "#     \"\"\"\n",
    "#     BN_bnc: BatchNorm1d on hidden feature with (B,N,C) dimension\n",
    "#     \"\"\"\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, N, C = x.shape\n",
    "#         x = x.reshape(B * N, C)  # (B,N,C) -> (B*N,C)\n",
    "#         x = super().forward(x)   # apply batch normalization\n",
    "#         x = x.reshape(B, N, C)   # (B*N,C) -> (B,N,C)\n",
    "#         return x\n",
    "\n",
    "\n",
    "\n",
    "# 1. Create a class that inherits from nn.Module\n",
    "class MLPBlock(nn.Module):\n",
    "    \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 dropout:float=0.3): # Dropout from Table 3 for ViT-Base\n",
    "        super().__init__()\n",
    "        \n",
    "        weight_decay=1e-4\n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "        # 4. Create the Multilayer perceptron (MLP) layer(s)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=mlp_size),\n",
    "#             nn.BatchNorm1d(mlp_size),\n",
    "            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n",
    "                      out_features=embedding_dim), # take back to embedding_dim\n",
    "#             nn.BatchNorm1d(embedding_dim),\n",
    "            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"\n",
    "        )\n",
    "    \n",
    "    # 5. Create a forward() method to pass the data throguh the layers\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa408b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"Creates a Transformer Encoder block.\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base #12\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 mlp_dropout:float=0.3, # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
    "                 attn_dropout:float=0): # Amount of dropout for attention layers\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create MSA block (equation 2)\n",
    "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     attn_dropout=attn_dropout)\n",
    "        \n",
    "        # 4. Create MLP block (equation 3)\n",
    "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
    "                                   mlp_size=mlp_size,\n",
    "                                   dropout=mlp_dropout)\n",
    "        \n",
    "    # 5. Create a forward() method  \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 6. Create residual connection for MSA block (add the input to the output)\n",
    "        x =  self.msa_block(x) + x \n",
    "        \n",
    "        # 7. Create residual connection for MLP block (add the input to the output)\n",
    "        x = self.mlp_block(x) + x \n",
    "        \n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dded45ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a ViT class that inherits from nn.Module\n",
    "class ViT(nn.Module):\n",
    "    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 img_size:int=224, # Training resolution from Table 3 in ViT paper\n",
    "                 in_channels:int=3, # Number of channels in input image\n",
    "                 patch_size:int=16, # Patch size 8\n",
    "                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base 4\n",
    "                 embedding_dim:int= 768, # Hidden size D from Table 1 for ViT-Base 128\n",
    "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
    "                 num_heads:int=12, # Heads from Table 1 for ViT-Base 4\n",
    "                 attn_dropout:float=0.2, # Dropout for attention projection\n",
    "                 mlp_dropout:float=0.2, # Dropout for dense/MLP layers \n",
    "                 embedding_dropout:float=0.2, # Dropout for patch and position embeddings\n",
    "                 num_classes:int=1000): # Default for ImageNet but can customize this\n",
    "        super().__init__() # don't forget the super().__init__()!\n",
    "        \n",
    "        # 3. Make the image size is divisble by the patch size \n",
    "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
    "        \n",
    "        # 4. Calculate number of patches (height * width/patch^2)\n",
    "        self.num_patches = (img_size * img_size) // patch_size**2\n",
    "                 \n",
    "        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
    "                                            requires_grad=True)\n",
    "        \n",
    "        # 6. Create learnable position embedding\n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
    "                                               requires_grad=True)\n",
    "                \n",
    "        # 7. Create embedding dropout value\n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "        \n",
    "        # 8. Create patch embedding layer\n",
    "        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
    "                                              patch_size=patch_size,\n",
    "                                              embedding_dim=embedding_dim)\n",
    "        \n",
    "        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential()) \n",
    "        # Note: The \"*\" means \"all\"\n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                            num_heads=num_heads,\n",
    "                                                                            mlp_size=mlp_size,\n",
    "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
    "       \n",
    "        # 10. Create classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim, \n",
    "                      out_features=num_classes)\n",
    "        )\n",
    "        self.softmax=torch.nn.Softmax()\n",
    "    \n",
    "    # 11. Create a forward() method\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 12. Get batch size\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # 13. Create class token embedding and expand it to match the batch size (equation 1)\n",
    "        class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n",
    "\n",
    "        # 14. Create patch embedding (equation 1)\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        # 15. Concat class embedding and patch embedding (equation 1)\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "\n",
    "        # 16. Add position embedding to patch embedding (equation 1) \n",
    "        x = self.position_embedding + x\n",
    "\n",
    "        # 17. Run embedding dropout (Appendix B.1)\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 & 3)\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # 19. Put 0 index logit through classifier (equation 4)\n",
    "        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n",
    "        \n",
    "        \n",
    "#         x=self.softmax(x)\n",
    "\n",
    "        return x       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5496ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train our MOdel\n",
    "\n",
    "# Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi)\n",
    "vit = ViT(num_classes=len(class_names))\n",
    "vit.load_state_dict(torch.load('E:/Deep learning/Project/code_data/Image-Classification-Using-Vision-transformer-main/weights/last_2/8.pt'), strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda30f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Predict on a target image with a target model\n",
    "# Function created in: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set\n",
    "def pred_and_plot_image(\n",
    "    model: torch.nn.Module,\n",
    "    class_names: List[str],\n",
    "    image_path: str,\n",
    "    image_size: Tuple[int, int] = (224, 224),\n",
    "    transform: torchvision.transforms = None,\n",
    "    device: torch.device = device,\n",
    "):\n",
    "    \"\"\"Predicts on a target image with a target model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): A trained (or untrained) PyTorch model to predict on an image.\n",
    "        class_names (List[str]): A list of target classes to map predictions to.\n",
    "        image_path (str): Filepath to target image to predict on.\n",
    "        image_size (Tuple[int, int], optional): Size to transform target image to. Defaults to (224, 224).\n",
    "        transform (torchvision.transforms, optional): Transform to perform on image. Defaults to None which uses ImageNet normalization.\n",
    "        device (torch.device, optional): Target device to perform prediction on. Defaults to device.\n",
    "    \"\"\"\n",
    "\n",
    "    # Open image\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "\n",
    "    # Create transformation for image (if one doesn't exist)\n",
    "    if transform is not None:\n",
    "        image_transform = transform\n",
    "    else:\n",
    "        image_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.556, 0.447, 0.335], std=[0.231, 0.242, 0.238]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    ### Predict on image ###\n",
    "\n",
    "    # Make sure the model is on the target device\n",
    "    model.to(device)\n",
    "\n",
    "    # Turn on model evaluation mode and inference mode\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n",
    "        transformed_image = image_transform(img).unsqueeze(dim=0)\n",
    "\n",
    "        # Make a prediction on image with an extra dimension and send it to the target device\n",
    "        target_image_pred = model(transformed_image.to(device))\n",
    "\n",
    "    # Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
    "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
    "\n",
    "    # Convert prediction probabilities -> prediction labels\n",
    "    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
    "\n",
    "    # Plot image with predicted label and probability\n",
    "#     plt.figure()\n",
    "#     plt.imshow(img)\n",
    "#     plt.title(\n",
    "#         f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f} | {target_image_pred_probs}\"\n",
    "#     )\n",
    "#     plt.axis(False)\n",
    "    return class_names[target_image_pred_label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d8994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "custom_image_path = \"E:/Deep learning/Project/OTHER_DATA/2/archive/food-101/food-101/sub_v2/subset_test_v2/ice_cream/\"\n",
    "\n",
    "garlic_bread=0\n",
    "hot_dog = 0\n",
    "ice_cream= 0\n",
    "omelette= 0 \n",
    "pizza=0\n",
    "\n",
    "paths=os.listdir(custom_image_path)\n",
    "for i in paths:\n",
    "    img_path= os.path.join(custom_image_path,i)\n",
    "\n",
    "    # Predict on custom image\n",
    "    classname = pred_and_plot_image(model=vit,\n",
    "                        image_path=img_path,\n",
    "                        class_names=class_names)\n",
    "    \n",
    "    if classname=='garlic_bread':\n",
    "        garlic_bread=garlic_bread+1\n",
    "    elif classname=='hot_dog':\n",
    "        hot_dog=hot_dog+1\n",
    "    elif classname=='ice_cream':\n",
    "        ice_cream=ice_cream+1\n",
    "    elif classname=='omelette':\n",
    "        omelette=omelette+1\n",
    "    else:\n",
    "        pizza=pizza+1\n",
    "        \n",
    "print(\"Total Pizza GT=\",40)\n",
    "print(\"garlic_bread=\",garlic_bread)\n",
    "print(\"hot_dog=\",hot_dog)\n",
    "print(\"ice_cream=\",ice_cream)\n",
    "print(\"omelette=\",omelette)\n",
    "print(\"pizza=\",pizza)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fe4bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "custom_image_path = \"E:/Deep learning/Project/OTHER_DATA/2/archive/food-101/food-101/sub_v2/subset_test_v2/garlic_bread/\"\n",
    "\n",
    "garlic_bread=0\n",
    "hot_dog = 0\n",
    "ice_cream= 0\n",
    "omelette= 0 \n",
    "pizza=0\n",
    "\n",
    "paths=os.listdir(custom_image_path)\n",
    "for i in paths:\n",
    "    img_path= os.path.join(custom_image_path,i)\n",
    "\n",
    "    # Predict on custom image\n",
    "    classname = pred_and_plot_image(model=vit,\n",
    "                        image_path=img_path,\n",
    "                        class_names=class_names)\n",
    "    \n",
    "    if classname=='garlic_bread':\n",
    "        garlic_bread=garlic_bread+1\n",
    "    elif classname=='hot_dog':\n",
    "        hot_dog=hot_dog+1\n",
    "    elif classname=='ice_cream':\n",
    "        ice_cream=ice_cream+1\n",
    "    elif classname=='omelette':\n",
    "        omelette=omelette+1\n",
    "    else:\n",
    "        pizza=pizza+1\n",
    "        \n",
    "print(\"Total garlic_bread GT=\",40)\n",
    "print(\"garlic_bread=\",garlic_bread)\n",
    "print(\"hot_dog=\",hot_dog)\n",
    "print(\"ice_cream=\",ice_cream)\n",
    "print(\"omelette=\",omelette)\n",
    "print(\"pizza=\",pizza)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "custom_image_path = \"E:/Deep learning/Project/OTHER_DATA/2/archive/food-101/food-101/sub_v2/subset_test_v2/omelette/\"\n",
    "\n",
    "garlic_bread=0\n",
    "hot_dog = 0\n",
    "ice_cream= 0\n",
    "omelette= 0 \n",
    "pizza=0\n",
    "\n",
    "paths=os.listdir(custom_image_path)\n",
    "for i in paths:\n",
    "    img_path= os.path.join(custom_image_path,i)\n",
    "\n",
    "    # Predict on custom image\n",
    "    classname = pred_and_plot_image(model=vit,\n",
    "                        image_path=img_path,\n",
    "                        class_names=class_names)\n",
    "    \n",
    "    if classname=='garlic_bread':\n",
    "        garlic_bread=garlic_bread+1\n",
    "    elif classname=='hot_dog':\n",
    "        hot_dog=hot_dog+1\n",
    "    elif classname=='ice_cream':\n",
    "        ice_cream=ice_cream+1\n",
    "    elif classname=='omelette':\n",
    "        omelette=omelette+1\n",
    "    else:\n",
    "        pizza=pizza+1\n",
    "        \n",
    "print(\"Total omelette GT=\",40)\n",
    "print(\"garlic_bread=\",garlic_bread)\n",
    "print(\"hot_dog=\",hot_dog)\n",
    "print(\"ice_cream=\",ice_cream)\n",
    "print(\"omelette=\",omelette)\n",
    "print(\"pizza=\",pizza)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9a910d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f8c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "custom_image_path = \"E:/Deep learning/Project/OTHER_DATA/2/archive/food-101/food-101/sub_v2/subset_test_v2/hot_dog/\"\n",
    "\n",
    "\n",
    "garlic_bread=0\n",
    "hot_dog = 0\n",
    "ice_cream= 0\n",
    "omelette= 0 \n",
    "pizza=0\n",
    "\n",
    "paths=os.listdir(custom_image_path)\n",
    "for i in paths:\n",
    "    img_path= os.path.join(custom_image_path,i)\n",
    "\n",
    "    # Predict on custom image\n",
    "    classname = pred_and_plot_image(model=vit,\n",
    "                        image_path=img_path,\n",
    "                        class_names=class_names)\n",
    "    \n",
    "    if classname=='garlic_bread':\n",
    "        garlic_bread=garlic_bread+1\n",
    "    elif classname=='hot_dog':\n",
    "        hot_dog=hot_dog+1\n",
    "    elif classname=='ice_cream':\n",
    "        ice_cream=ice_cream+1\n",
    "    elif classname=='omelette':\n",
    "        omelette=omelette+1\n",
    "    else:\n",
    "        pizza=pizza+1\n",
    "        \n",
    "print(\"Total hot_dog GT=\",40)\n",
    "print(\"garlic_bread=\",garlic_bread)\n",
    "print(\"hot_dog=\",hot_dog)\n",
    "print(\"ice_cream=\",ice_cream)\n",
    "print(\"omelette=\",omelette)\n",
    "print(\"pizza=\",pizza)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b077c01a",
   "metadata": {},
   "source": [
    "## Metrics calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5988955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "def pred_and_plot_image(\n",
    "    model: torch.nn.Module,\n",
    "    class_names: List[str],\n",
    "    image_path: str,\n",
    "    GT:str,\n",
    "    image_size: Tuple[int, int] = (224, 224),\n",
    "    transform: torchvision.transforms = None,\n",
    "    device: torch.device = device,\n",
    "):\n",
    "\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    if transform is not None:\n",
    "        image_transform = transform\n",
    "    else:\n",
    "        image_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.556, 0.447, 0.335], std=[0.231, 0.242, 0.238]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        # Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n",
    "        transformed_image = image_transform(img).unsqueeze(dim=0)\n",
    "\n",
    "        # Make a prediction on image with an extra dimension and send it to the target device\n",
    "        target_image_pred = model(transformed_image.to(device))\n",
    "\n",
    "    # Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
    "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
    "\n",
    "    # Convert prediction probabilities -> prediction labels\n",
    "    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
    "    print(\"cLASS NAMES=\",class_names)\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.title(\n",
    "        f\" GT:{GT} | Pred : {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f} | {target_image_pred_probs}\"\n",
    "    )\n",
    "    plt.axis(False)\n",
    "    plt.show()\n",
    "\n",
    "    return target_image_pred_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "predictions=[]\n",
    "\n",
    "class_names = ['garlic_bread', 'hot_dog', 'ice_cream', 'omelette', 'pizza']\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(vit, img_path, class_GT):\n",
    "    global predictions\n",
    "  \n",
    "    predicted_label = pred_and_plot_image(model=vit,\n",
    "                        image_path=img_path,\n",
    "                        class_names=class_names,GT=class_GT)\n",
    "    \n",
    "    predictions.append(predicted_label.item())\n",
    "    \n",
    "\n",
    "\n",
    "class_names=['garlic_bread', 'hot_dog', 'ice_cream', 'omelette', 'pizza']\n",
    "# test_data_path = \"E:/Deep learning/Project/OTHER_DATA/2/archive/food-101/food-101/sub_v2/subset_test_v2/\"\n",
    "test_data_path = \"E:/Deep learning/Project/OTHER_DATA/2/archive/food-101/food-101/sub_v2/subset_train_v2/\"\n",
    "\n",
    "\n",
    "ground_truth=[]\n",
    "for class_label, class_name in enumerate(class_names):\n",
    "    class_folder_path = os.path.join(test_data_path, class_name)\n",
    "    print(\"Class name=\",class_name)\n",
    "    # Loop through each image in the class folder\n",
    "    for image_name in os.listdir(class_folder_path):\n",
    "        image_path = os.path.join(class_folder_path, image_name)\n",
    "\n",
    "        # Predict\n",
    "        predict(vit, image_path, class_name)\n",
    "        ground_truth.append(class_label)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f9685",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predictions=\",predictions)\n",
    "print(\"Ground truth =\",ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d9d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(ground_truth, predictions)\n",
    "\n",
    "# Calculate metrics for each class\n",
    "classwise_accuracy = accuracy_score(ground_truth, predictions)\n",
    "precision = precision_score(ground_truth, predictions, average=None)\n",
    "recall = recall_score(ground_truth, predictions, average=None)\n",
    "f1 = f1_score(ground_truth, predictions, average=None)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"\\nClass-wise Accuracy:\", classwise_accuracy)\n",
    "print(\"Precision for each class:\", precision)\n",
    "print(\"Recall for each class:\", recall)\n",
    "print(\"F1 Score for each class:\", f1)\n",
    "\n",
    "overall_accuracy = accuracy_score(ground_truth, predictions)\n",
    "class_names=['garlic_bread', 'hot_dog', 'ice_cream', 'omelette', 'pizza']\n",
    "print(\"Overall Accuracy:\", overall_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dc4794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c96ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882d105f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbd431d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ef195e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a107b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6193f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aa3d61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
